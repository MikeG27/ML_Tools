{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R45gVkjW7dSZ"
      },
      "source": [
        "<h2>Welcome to GLiNER-Studio!</h2>\n",
        "<h3><a href=\"https://www.loom.com/share/a0b54b3509b74a5f928ae7fd9114ddef?sid=ed7df24c-25ba-4bb7-a723-798bcb08609e\" target=\"_blank\">Video Quickstart</a></h3>\n",
        "<h3>With GLiNER-Studio, you can effortlessly fine-tune any GLiNER-based model on your custom dataset to handle the following tasks:</h3>\n",
        "<ol>\n",
        "    <li><b>Named Entity Recognition (NER):</b> Identifies and categorizes entities such as names, organizations, dates, and other specific items in the text.</li>\n",
        "    <li><b>Relation Extraction:</b> Detects and classifies relationships between entities within the text.</li>\n",
        "    <li><b>Summarization:</b> Extract the most important sentences that summarize the input text, capturing the essential information.</li>\n",
        "    <li><b>Sentiment Extraction:</b> Identify parts of the text that signal a positive, negative, or neutral sentiment.</li>\n",
        "    <li><b>Key-Phrase Extraction:</b> Identifies and extracts important phrases and keywords from the text.</li>\n",
        "    <li><b>Question-answering:</b> Finding an answer in the text given a question.</li>\n",
        "    <li><b>Open Information Extraction:</b> Extracts pieces of text given an open prompt from a user, for example, product description extraction.</li>\n",
        "    <li><b>Text Cleaning:</b> Clear the text from unnecessary parts according to the prompt.</li>\n",
        "</ol>\n",
        "<h3>Remember, information extraction is not just about data; it's about insights. Let's uncover those insights together!</h3>\n",
        "\n",
        "<!-- Links Section -->\n",
        "<p>\n",
        "    <a href=\"https://www.knowledgator.com/\" target=\"_blank\">Visit our website</a> |\n",
        "    <a href=\"https://www.linkedin.com/company/knowledgator/\" target=\"_blank\">Follow on LinkedIn</a> |\n",
        "    <a href=\"https://huggingface.co/knowledgator/\" target=\"_blank\">Hugging Face Profile</a> |\n",
        "    <a href=\"https://twitter.com/knowledgator\" target=\"_blank\">Follow on X</a> |\n",
        "    <a href=\"https://blog.knowledgator.com/\" target=\"_blank\">Follow on Medium</a> |\n",
        "    <a href=\"https://discord.com/invite/dkyeAgs9DG\" target=\"_blank\">Join our Discord</a>\n",
        "</p>\n",
        "\n",
        "<h3>Please, cite if you have used GLiNER-Studio to finetune your model:</h3>\n",
        "<pre>\n",
        "'''\n",
        "@misc{stepanov2024gliner,\n",
        "      title={GLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks},\n",
        "      author={Ihor Stepanov and Mykhailo Shtopko},\n",
        "      year={2024},\n",
        "      eprint={2406.12925},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}\n",
        "}\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KeBDnZK9KHcz",
        "outputId": "490c5ea1-d0a4-4904-931c-37cf64bb6d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting gliner\n",
            "  Downloading gliner-0.2.16-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.4 (from gradio)\n",
            "  Downloading gradio_client-1.5.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.5)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.5.4->gradio) (14.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gliner) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.11/dist-packages (from gliner) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gliner) (4.67.1)\n",
            "Collecting onnxruntime (from gliner)\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from gliner) (0.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->gliner) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->gliner) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->gliner) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->gliner) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->gliner) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->gliner) (0.5.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Collecting coloredlogs (from onnxruntime->gliner)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->gliner) (24.12.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime->gliner) (4.25.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.12.0-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.4-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gliner-0.2.16-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, humanfriendly, ffmpy, aiofiles, starlette, coloredlogs, safehttpx, onnxruntime, gradio-client, fastapi, gradio, gliner\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 coloredlogs-15.0.1 fastapi-0.115.6 ffmpy-0.5.0 gliner-0.2.16 gradio-5.12.0 gradio-client-1.5.4 humanfriendly-10.0 markupsafe-2.1.5 onnxruntime-1.20.1 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
            "Downloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.2.1\n",
            "    Uninstalling accelerate-1.2.1:\n",
            "      Successfully uninstalled accelerate-1.2.1\n",
            "Successfully installed accelerate-1.3.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "# @title Installations\n",
        "!pip install gradio gliner\n",
        "!pip install accelerate -U\n",
        "!pip install transformers huggingface_hub\n",
        "\n",
        "import gradio as gr\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import torch\n",
        "from gliner import GLiNER\n",
        "from gliner import GLiNERConfig, GLiNER\n",
        "from gliner.training import Trainer, TrainingArguments\n",
        "from gliner.data_processing.collator import DataCollatorWithPadding\n",
        "from gliner.utils import load_config_as_namespace\n",
        "from gliner.data_processing import WordsSplitter, GLiNERDataset\n",
        "\n",
        "if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "# List of available models\n",
        "AVAILABLE_MODELS = [\n",
        "    \"knowledgator/gliner-multitask-large-v0.5\",\n",
        "    \"urchade/gliner_multi-v2.1\",\n",
        "    \"urchade/gliner_large_bio-v0.1\",\n",
        "    \"numind/NuNER_Zero\",\n",
        "    \"EmergentMethods/gliner_medium_news-v2.1\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cBUCEExkKOlF"
      },
      "outputs": [],
      "source": [
        "# @title #Upload your sentences examples\n",
        "import gradio as gr\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Ensure the /data directory exists\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Function to save the uploaded file\n",
        "def save_file(uploaded_file):\n",
        "    if uploaded_file is None:\n",
        "        return \"No file uploaded.\"\n",
        "\n",
        "    # Define the path where the file will be saved\n",
        "    save_path = os.path.join(\"data\")\n",
        "\n",
        "    try:\n",
        "        # Save the file with the new name\n",
        "        shutil.copy(uploaded_file.name, save_path)\n",
        "        return f\"File saved to {save_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as loader:\n",
        "    gr.Markdown(\"# File Upload and Save Example\")\n",
        "\n",
        "    # File uploader component\n",
        "    file_uploader = gr.File(label=\"Upload your file here\")\n",
        "\n",
        "    # Button to trigger the file save function\n",
        "    save_button = gr.Button(\"Save File\")\n",
        "\n",
        "    # Output textbox to show the result\n",
        "    output = gr.Textbox(label=\"Result\")\n",
        "\n",
        "    # Link the button to the save_file function\n",
        "    save_button.click(fn=save_file, inputs=file_uploader, outputs=output)\n",
        "\n",
        "# Launch the interface\n",
        "loader.launch(share=True, inline=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgjpcmnJKf0d"
      },
      "source": [
        "**If you don't have the final dataset, upload sentences examples to auto annotate them, otherwise, upload your file and skip until validation.**\n",
        "\n",
        "**Run the cell above ☝️**\n",
        "\n",
        "**Or you can write your custom function for loading of a dataset 👇**\n",
        "\\\n",
        "\\\n",
        "**🛑 If you have already annotated dataset, please scroll down, there is a way to load it directly 🛑**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GJ99KyV11MOb"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "sentences = [\n",
        "        \"IBM Watson defeated human champions in the game of Jeopardy!\",\n",
        "        \"The Amazon rainforest is known as the lungs of the Earth.\",\n",
        "        \"Sydney Opera House is an iconic symbol of Australia.\",\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"A journey of a thousand miles begins with a single step.\",\n",
        "        \"To be or not to be, that is the question.\",\n",
        "        \"All that glitters is not gold.\",\n",
        "        \"The early bird catches the worm.\",\n",
        "        \"Google is building a new office in New York.\",\n",
        "        \"The movie Inception was directed by Christopher Nolan.\"\n",
        "        \"Jeff Bezos founded Amazon in 1994.\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BbhM8OO3MNI0"
      },
      "outputs": [],
      "source": [
        "# @title Prepare Data for Manual Annotation\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize the input text into a list of tokens.\"\"\"\n",
        "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
        "\n",
        "def prepare_data_for_manual_annotation(sentences):\n",
        "  annotated_data = []\n",
        "  for text in sentences:\n",
        "    annotated_data.append({\"tokenized_text\": tokenize_text(text), \"ner\": [], \"validated\": False})\n",
        "  with open(\"data/annotated_data.json\", \"wt\") as file:\n",
        "    json.dump(annotated_data, file)\n",
        "\n",
        "prepare_data_for_manual_annotation(sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M16R6bFow5Lt",
        "outputId": "8d88ac74-da54-457e-c337-46ac897eb71c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7a046546d9cef5bac5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7a046546d9cef5bac5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# @title Auto Annotation\n",
        "\n",
        "# Provided post-processing functions\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize the input text into a list of tokens.\"\"\"\n",
        "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
        "\n",
        "def transform_data(data):\n",
        "    tokens = tokenize_text(data['text'])\n",
        "    spans = []\n",
        "\n",
        "    for entity in data['entities']:\n",
        "        entity_tokens = tokenize_text(entity['word'])\n",
        "        entity_length = len(entity_tokens)\n",
        "\n",
        "        # Find the start and end indices of each entity in the tokenized text\n",
        "        for i in range(len(tokens) - entity_length + 1):\n",
        "            if tokens[i:i + entity_length] == entity_tokens:\n",
        "                spans.append([i, i + entity_length - 1, entity['entity']])\n",
        "                break\n",
        "\n",
        "    return {\"tokenized_text\": tokens, \"ner\": spans, \"validated\": False}\n",
        "\n",
        "def merge_entities(entities):\n",
        "    if not entities:\n",
        "        return []\n",
        "    merged = []\n",
        "    current = entities[0]\n",
        "    for next_entity in entities[1:]:\n",
        "        if next_entity['entity'] == current['entity'] and (next_entity['start'] == current['end'] + 1 or next_entity['start'] == current['end']):\n",
        "            current['word'] += ' ' + next_entity['word']\n",
        "            current['end'] = next_entity['end']\n",
        "        else:\n",
        "            merged.append(current)\n",
        "            current = next_entity\n",
        "    merged.append(current)\n",
        "    return merged\n",
        "\n",
        "def annotate_text(\n",
        "    model, text, labels: List[str], threshold: float, nested_ner: bool\n",
        ") -> Dict:\n",
        "    labels = [label.strip() for label in labels]\n",
        "    r = {\n",
        "        \"text\": text,\n",
        "        \"entities\": [\n",
        "            {\n",
        "                \"entity\": entity[\"label\"],\n",
        "                \"word\": entity[\"text\"],\n",
        "                \"start\": entity[\"start\"],\n",
        "                \"end\": entity[\"end\"],\n",
        "                \"score\": 0,\n",
        "            }\n",
        "            for entity in model.predict_entities(\n",
        "                text, labels, flat_ner=not nested_ner, threshold=threshold\n",
        "            )\n",
        "        ],\n",
        "    }\n",
        "    r[\"entities\"] = merge_entities(r[\"entities\"])\n",
        "    return transform_data(r)\n",
        "\n",
        "class AutoAnnotator:\n",
        "    def __init__(\n",
        "        self, model: int = \"knowledgator/gliner-multitask-large-v0.5\",\n",
        "        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        ) -> None:\n",
        "\n",
        "        self.model = GLiNER.from_pretrained(model).to(device)\n",
        "        self.annotated_data = []\n",
        "        self.stat = {\n",
        "            \"total\": None,\n",
        "            \"current\": -1\n",
        "        }\n",
        "\n",
        "    def auto_annotate(\n",
        "            self, data: List[str], labels: List[str],\n",
        "            prompt: Union[str, List[str]] = None, threshold: float = 0.5, nested_ner: bool = False\n",
        "            ) -> List[Dict]:\n",
        "        self.stat[\"total\"] = len(data)\n",
        "        self.stat[\"current\"] = -1  # Reset current progress\n",
        "        for text in data:\n",
        "            self.stat[\"current\"] += 1\n",
        "            if isinstance(prompt, list):\n",
        "                prompt_text = random.choice(prompt)\n",
        "            else:\n",
        "                prompt_text = prompt\n",
        "            text = f\"{prompt_text}\\n{text}\" if prompt_text else text\n",
        "\n",
        "            annotation = annotate_text(self.model, text, labels, threshold, nested_ner)\n",
        "\n",
        "            if not annotation[\"ner\"]:  # If no entities identified\n",
        "                annotation = {\"tokenized_text\": tokenize_text(text), \"ner\": [], \"validated\": False}\n",
        "\n",
        "            self.annotated_data.append(annotation)\n",
        "        return self.annotated_data\n",
        "\n",
        "# Define a global annotator\n",
        "annotator = None\n",
        "\n",
        "# Function to annotate data\n",
        "def annotate(model, labels, threshold, prompt):\n",
        "    global annotator\n",
        "    try:\n",
        "        labels = [label.strip() for label in labels.split(\",\")]\n",
        "        annotator = AutoAnnotator(model)\n",
        "        annotated_data = annotator.auto_annotate(sentences, labels, prompt, threshold)\n",
        "        with open(\"data/annotated_data.json\", \"wt\") as file:\n",
        "            json.dump(annotated_data, file)\n",
        "        return \"Successfully annotated and saved as data/annotated_data.json\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "\n",
        "# Gradio interface\n",
        "with gr.Blocks() as annotator_interface:\n",
        "    labels = gr.Textbox(label=\"Labels\", placeholder=\"Enter your comma-separated labels here\", scale=2)\n",
        "    model = gr.Dropdown(label=\"Choose the model which will be used for annotation\", choices=AVAILABLE_MODELS)\n",
        "    threshold = gr.Slider(0, 1, value=0.3, step=0.01, label=\"Threshold\", info=\"Lower the threshold to increase how many entities get predicted.\")\n",
        "    prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Enter your annotation prompt here\", scale=2)\n",
        "    submit_btn = gr.Button(\"Annotate data\")\n",
        "    output_info = gr.Textbox(label=\"Processing info:\")\n",
        "\n",
        "    submit_btn.click(fn=annotate, inputs=[model, labels, threshold, prompt], outputs=output_info)\n",
        "\n",
        "annotator_interface.launch(inline=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXJx3cMnLo17"
      },
      "source": [
        "**Run the cell above ☝️ to auto-annotate the dataset with one of the available GLiNER models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynm09JX1OUAt"
      },
      "source": [
        "**Run the cell above ☝️ to load already annotated dataset.**\n",
        "\n",
        "**⚡ Skip it if you auto-annotated dataset ⚡**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5DOfHuPgxB8i",
        "outputId": "ff73dd3d-23d8-4945-8065-d27f53ddeddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7e246b4553a26ff639.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7e246b4553a26ff639.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# @title Dataset Viewer\n",
        "\n",
        "class DynamicDataset:\n",
        "    def __init__(\n",
        "            self, data: List[Dict[str, Union[List[Union[int, str]], bool]]]\n",
        "                 ) -> None:\n",
        "        self.data = data\n",
        "        self.data_len = len(self.data)\n",
        "        self.current = -1\n",
        "        for example in self.data:\n",
        "            if not \"validated\" in example.keys():\n",
        "                example[\"validated\"] = False\n",
        "\n",
        "    def next_example(self):\n",
        "        self.current += 1\n",
        "        if self.current > self.data_len-1:\n",
        "          self.current = self.data_len -1\n",
        "        elif self.current < 0:\n",
        "          self.current = 0\n",
        "\n",
        "    def previous_example(self):\n",
        "        self.current -= 1\n",
        "        if self.current > self.data_len-1:\n",
        "          self.current = self.data_len -1\n",
        "        elif self.current < 0:\n",
        "          self.current = 0\n",
        "\n",
        "    def example_by_id(self, id):\n",
        "        self.current = id\n",
        "        if self.current > self.data_len-1:\n",
        "          self.current = self.data_len -1\n",
        "        elif self.current < 0:\n",
        "          self.current = 0\n",
        "\n",
        "    def validate(self):\n",
        "        self.data[self.current][\"validated\"] = True\n",
        "\n",
        "    def load_current_example(self):\n",
        "        return self.data[self.current]\n",
        "\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize the input text into a list of tokens.\"\"\"\n",
        "    return re.findall(r'\\w+(?:[-_]\\w+)*|\\S', text)\n",
        "\n",
        "\n",
        "def join_tokens(tokens):\n",
        "    # Joining tokens with space, but handling special characters correctly\n",
        "    text = \"\"\n",
        "    for token in tokens:\n",
        "        if token in {\",\", \".\", \"!\", \"?\", \":\", \";\", \"...\"}:\n",
        "            text = text.rstrip() + token\n",
        "        else:\n",
        "            text += \" \" + token\n",
        "    return text.strip()\n",
        "\n",
        "def prepare_for_highlight(data):\n",
        "    tokens = data[\"tokenized_text\"]\n",
        "    ner = data[\"ner\"]\n",
        "\n",
        "    highlighted_text = []\n",
        "    current_entity = None\n",
        "    entity_tokens = []\n",
        "    normal_tokens = []\n",
        "\n",
        "    for idx, token in enumerate(tokens):\n",
        "        # Check if the current token is the start of a new entity\n",
        "        if current_entity is None or idx > current_entity[1]:\n",
        "            if entity_tokens:\n",
        "                highlighted_text.append((\" \".join(entity_tokens), current_entity[2]))\n",
        "                entity_tokens = []\n",
        "            current_entity = next((entity for entity in ner if entity[0] == idx), None)\n",
        "\n",
        "        # If current token is part of an entity\n",
        "        if current_entity and current_entity[0] <= idx <= current_entity[1]:\n",
        "            if normal_tokens:\n",
        "                highlighted_text.append((\" \".join(normal_tokens), None))\n",
        "                normal_tokens = []\n",
        "            entity_tokens.append(token + \" \")\n",
        "        else:\n",
        "            if entity_tokens:\n",
        "                highlighted_text.append((\" \".join(entity_tokens), current_entity[2]))\n",
        "                entity_tokens = []\n",
        "            normal_tokens.append(token + \" \")\n",
        "\n",
        "    # Append any remaining tokens\n",
        "    if entity_tokens:\n",
        "        highlighted_text.append((\" \".join(entity_tokens), current_entity[2]))\n",
        "    if normal_tokens:\n",
        "        highlighted_text.append((\" \".join(normal_tokens), None))\n",
        "    # Clean up spaces before punctuation\n",
        "    cleaned_highlighted_text = []\n",
        "    for text, label in highlighted_text:\n",
        "        cleaned_text = re.sub(r'\\s(?=[,\\.!?…:;])', '', text)\n",
        "        cleaned_highlighted_text.append((cleaned_text, label))\n",
        "\n",
        "    return cleaned_highlighted_text\n",
        "\n",
        "def extract_tokens_and_labels(data: List[Dict[str, Union[str, None]]]) -> Dict[str, Union[List[str], List[Tuple[int, int, str]]]]:\n",
        "    tokens = []\n",
        "    ner = []\n",
        "\n",
        "    token_start_idx = 0\n",
        "\n",
        "    for entry in data:\n",
        "        char = entry['token']\n",
        "        label = entry['class_or_confidence']\n",
        "\n",
        "        # Tokenize the current text chunk\n",
        "        token_list = tokenize_text(char)\n",
        "\n",
        "        # Append tokens to the main tokens list\n",
        "        tokens.extend(token_list)\n",
        "\n",
        "        if label:\n",
        "            token_end_idx = token_start_idx + len(token_list) - 1\n",
        "            ner.append((token_start_idx, token_end_idx, label))\n",
        "\n",
        "        token_start_idx += len(token_list)\n",
        "\n",
        "    return tokens, ner\n",
        "\n",
        "def update_example(data):\n",
        "    global dynamic_dataset\n",
        "    tokens, ner = extract_tokens_and_labels(data)\n",
        "    dynamic_dataset.data[dynamic_dataset.current][\"tokenized_text\"] = tokens\n",
        "    dynamic_dataset.data[dynamic_dataset.current][\"ner\"] = ner\n",
        "    return prepare_for_highlight(dynamic_dataset.load_current_example())\n",
        "\n",
        "def validate_example():\n",
        "    global dynamic_dataset\n",
        "    dynamic_dataset.data[dynamic_dataset.current][\"validated\"] = True\n",
        "    return [(\"The example was validated!\", None)]\n",
        "\n",
        "def next_example():\n",
        "    dynamic_dataset.next_example()\n",
        "    return prepare_for_highlight(dynamic_dataset.load_current_example()), dynamic_dataset.current\n",
        "\n",
        "def previous_example():\n",
        "    dynamic_dataset.previous_example()\n",
        "    return prepare_for_highlight(dynamic_dataset.load_current_example()), dynamic_dataset.current\n",
        "\n",
        "def save_dataset(inp):\n",
        "  with open(\"data/annotated_data.json\", \"wt\") as file:\n",
        "    json.dump(dynamic_dataset.data, file)\n",
        "  return [(\"The validates dataset was saved as data/annotated_data.json\", None)]\n",
        "\n",
        "with open(\"data/annotated_data.json\", 'rt') as dataset:\n",
        "  ANNOTATED_DATA = json.load(dataset)\n",
        "dynamic_dataset = DynamicDataset(ANNOTATED_DATA)\n",
        "DATASET_LEN = len(dynamic_dataset.data)\n",
        "\n",
        "with gr.Blocks() as dataset_viewer:\n",
        "    bar = gr.Slider(minimum=0, maximum=DATASET_LEN -1, step=1, label=\"Progress\", interactive=False)\n",
        "    with gr.Row():\n",
        "        previous_btn = gr.Button(\"Previous example\")\n",
        "        apply_btn = gr.Button(\"Apply changes\")\n",
        "        next_btn = gr.Button(\"Next example\")\n",
        "    validate_btn = gr.Button(\"Validate\")\n",
        "    save_btn = gr.Button(\"Save validated dataset\")\n",
        "\n",
        "    inp_box = gr.HighlightedText(value=None, interactive=True)\n",
        "    apply_btn.click(fn=update_example, inputs=inp_box, outputs=inp_box)\n",
        "    save_btn.click(fn=save_dataset, inputs=inp_box, outputs=inp_box)\n",
        "    validate_btn.click(fn=validate_example, inputs=None, outputs=inp_box)\n",
        "    next_btn.click(fn=next_example, inputs=None, outputs=[inp_box,bar])\n",
        "    previous_btn.click(fn=previous_example, inputs=None, outputs=[inp_box,bar])\n",
        "\n",
        "dataset_viewer.launch(share=True, inline=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l8NOxAETED6"
      },
      "source": [
        "Click `Next example` to access the first dataset item. Click `Apply changes` to save your annotation.\n",
        "\n",
        "\\\n",
        "\n",
        "To annotate an entity just highlight a text and write an appropriate label name\n",
        "\n",
        "\\\n",
        "⚡ Don't forget to `Save validated dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U0a6CQvIP_Bm",
        "collapsed": true,
        "outputId": "fe100f8b-8361-4d56-aaec-e3d20586c757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://01df963c29ca6519df.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://01df963c29ca6519df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# @title Train the model\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "def load_and_prepare_data(train_path, split_ratio):\n",
        "    if not os.path.exists(train_path):\n",
        "        raise FileNotFoundError(f\"The file {train_path} does not exist.\")\n",
        "\n",
        "    with open(train_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        "    train_data = data[:int(len(data) * split_ratio)]\n",
        "    test_data = data[int(len(data) * split_ratio):]\n",
        "    return train_data, test_data\n",
        "\n",
        "def create_models_directory():\n",
        "    if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
        "    global train_data, train_data\n",
        "    create_models_directory()\n",
        "\n",
        "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model = GLiNER.from_pretrained(model_name)\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    train_data, test_data = load_and_prepare_data(train_path, split_ratio)\n",
        "\n",
        "    with open(\"data/test.json\", \"wt\") as file:\n",
        "      json.dump(test_data, file)\n",
        "    print(f\"Training data size: {len(train_data)}, Testing data size: {len(test_data)}\")\n",
        "\n",
        "    train_dataset = transform_dataset(train_data, model.config, data_processor=model.data_processor)\n",
        "    test_dataset = transform_dataset(test_data, model.config, data_processor=model.data_processor)\n",
        "    data_collator = DataCollatorWithPadding(model.config)\n",
        "\n",
        "    if compile_model:\n",
        "        print(\"Compiling model for faster training...\")\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "        model.to(device)\n",
        "        model.compile_for_training()\n",
        "    else:\n",
        "        model.to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"models\",\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        others_lr=learning_rate,\n",
        "        others_weight_decay=weight_decay,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_ratio=0.1,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_steps=1000,\n",
        "        save_total_limit=10,\n",
        "        dataloader_num_workers=8,\n",
        "        use_cpu=(device == torch.device('cpu')),\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        tokenizer=model.data_processor.transformer_tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    model.save_pretrained(f\"models/{custom_model_name}\")\n",
        "\n",
        "    return \"Training completed successfully.\"\n",
        "\n",
        "# Gradio interface\n",
        "def gradio_train(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
        "    train_path = os.path.join(\"data\", train_path)\n",
        "    try:\n",
        "        return train_model(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "with gr.Blocks() as trainer_interface:\n",
        "    gr.Markdown(\"# GLiNER Training Interface\")\n",
        "    train_path = os.listdir(\"data\")\n",
        "    #local_models = [f\"models/{local_model}\" for local_model in os.listdir(\"models\")]\n",
        "    with gr.Row():\n",
        "      model_name = gr.Dropdown(label=\"Choose the parent model\", choices=AVAILABLE_MODELS, value=\"knowledgator/gliner-multitask-large-v0.5\")\n",
        "      custom_model_name = gr.Textbox(label=\"The name of your custom model\", placeholder=\"Enter the name of your new model\")\n",
        "      train_path = gr.Dropdown(label= \"Choose the dataset\",choices=train_path, value=\"annotated_data.json\")\n",
        "      split_ratio = gr.Slider(label=\"Train/Test Split Ratio\", minimum=0.1, maximum=0.9, step=0.1, value=0.9)\n",
        "    with gr.Row():\n",
        "      learning_rate = gr.Slider(label=\"Learning Rate\", minimum=1e-6, maximum=1e-4, step=1e-6, value=5e-6)\n",
        "      weight_decay = gr.Slider(label=\"Weight Decay\", minimum=0, maximum=0.1, step=0.01, value=0.01)\n",
        "      batch_size = gr.Slider(label=\"Batch Size\", minimum=1, maximum=128, step=1, value=8)\n",
        "      epochs = gr.Slider(label=\"Number of Epochs\", minimum=1, maximum=10, step=1, value=1)\n",
        "    compile_model = gr.Checkbox(label=\"Compile Model for Faster Training\", value=False)\n",
        "    train_btn = gr.Button(\"Start Training\")\n",
        "\n",
        "    output_info = gr.Textbox(label=\"Training Info\")\n",
        "\n",
        "    train_btn.click(fn=gradio_train, inputs=[model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model], outputs=output_info)\n",
        "\n",
        "trainer_interface.launch(inline=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_dataset(data, model_config, data_processor):\n",
        "    return GLiNERDataset(data, model_config, data_processor=data_processor)\n",
        "\n",
        "def preprocess_gliner(model_name, custom_model_name, train_path, split_ratio):\n",
        "    global train_data, train_data\n",
        "    create_models_directory()\n",
        "\n",
        "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model = GLiNER.from_pretrained(model_name)\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    train_data, test_data = load_and_prepare_data(train_path, split_ratio)\n",
        "\n",
        "    with open(\"data/test.json\", \"wt\") as file:\n",
        "      json.dump(test_data, file)\n",
        "    print(f\"Training data size: {len(train_data)}, Testing data size: {len(test_data)}\")\n",
        "\n",
        "    train_dataset = transform_dataset(train_data, model.config, data_processor=model.data_processor)\n",
        "    test_dataset = transform_dataset(test_data, model.config, data_processor=model.data_processor)\n",
        "    data_collator = DataCollatorWithPadding(model.config)\n",
        "\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "\n",
        "train_dataset, test_dataset, data_collator = preprocess_gliner(\n",
        "    model_name=\"knowledgator/gliner-multitask-large-v0.5\",\n",
        "    custom_model_name=\"my_model\",\n",
        "    train_path=\"data/annotated_data.json\",\n",
        "    split_ratio=0.7)"
      ],
      "metadata": {
        "id": "ypa9Hn0CuN8O",
        "outputId": "99fd92cf-3fcf-46dd-a7cc-355617ee7bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "a39ad6349a4e42eeab9cd9802f728abf",
            "30ba3ae0227e434f9bb01c93890fa812",
            "790a1064f9e74c8ba1a7a2a9d8aabf32",
            "c1f258dd5ba6476ba4dc83dd5d5a48bd",
            "d7af1df2c7224a428a035c31f827dc4f",
            "a60b93d225c04c3ea6b3d23a203bf5ad",
            "0e6b7aee22a5476da838ba6e57797382",
            "9162131de7644e2db20ab1c331d3f4db",
            "07b47cced2224025825f849c57567ea9",
            "dbc910057a8547f2b3ef6e5ab36fefbd",
            "8eb955ec9be045629e361bbddaacbc0c"
          ]
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a39ad6349a4e42eeab9cd9802f728abf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Training data size: 7, Testing data size: 3\n",
            "Collecting all entities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 1421.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of entity classes:  3\n",
            "Collecting all entities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 24576.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of entity classes:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.head()"
      ],
      "metadata": {
        "id": "kDidp1_nvxbj",
        "outputId": "b5f9ac8f-c201-4f9c-834e-2b5650af70b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GLiNERDataset' object has no attribute 'head'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a325ea520657>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'GLiNERDataset' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1WaKJOPVYDH"
      },
      "source": [
        "**Choose a model and set training parameters for your needs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rwxa-Eg671-H"
      },
      "outputs": [],
      "source": [
        "# @title Fast Mertics\n",
        "\n",
        "# Load the test.json file\n",
        "with open('data/test.json', 'r') as file:\n",
        "    test_data = json.load(file)\n",
        "\n",
        "with open('data/annotated_data.json', 'r') as file:\n",
        "    annotated_data = json.load(file)\n",
        "\n",
        "# Extract all labels from each example\n",
        "all_labels = []\n",
        "for example in annotated_data:\n",
        "    ner_data = example.get(\"ner\", [])\n",
        "    for entity in ner_data:\n",
        "        label = entity[2]  # Assuming the label is the third element in the entity list\n",
        "        if label not in all_labels:\n",
        "            all_labels.append(label)\n",
        "\n",
        "def evaluate_model(model_name):\n",
        "    model_path = f\"models/{model_name}\"\n",
        "    model = GLiNER.from_pretrained(model_path, load_tokenizer=True, local_files_only=True)\n",
        "\n",
        "    def get_for_one_path(test_dataset, entity_types):\n",
        "        # evaluate the model\n",
        "        results, f1 = model.evaluate(test_dataset, flat_ner=True, threshold=0.5, batch_size=12, entity_types=entity_types)\n",
        "        return results, f1\n",
        "\n",
        "    results, f1 = get_for_one_path(test_data, all_labels)\n",
        "    output_info = f\"F1 Score: {f1:.2f}\" + \"\\n\" + results\n",
        "    return output_info\n",
        "\n",
        "with gr.Blocks() as evaluation_interface:\n",
        "    gr.Markdown(\"# GLiNER Evaluation Interface\")\n",
        "    models = os.listdir(\"models\")\n",
        "    model_name = gr.Dropdown(label=\"Choose the model\", choices=models, value=models[0])\n",
        "\n",
        "    evaluate_btn = gr.Button(\"Evaluate Model\")\n",
        "    output_info = gr.Textbox(label=\"Evaluation Info\")\n",
        "\n",
        "    evaluate_btn.click(fn=evaluate_model, inputs=model_name, outputs=output_info)\n",
        "\n",
        "# Suppress all prints\n",
        "evaluation_interface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VWPkUahzN5fI"
      },
      "outputs": [],
      "source": [
        "# @title NER Inferance\n",
        "\n",
        "class Model:\n",
        "  def __init__(self) -> None:\n",
        "      self.previous_path = None\n",
        "      self.path = None\n",
        "      self.model = None\n",
        "  def get_model(self, path):\n",
        "      self.previous_path = None\n",
        "      self.path = path\n",
        "      if self.path != self.previous_path:\n",
        "          self.model = GLiNER.from_pretrained(f\"models/{self.path}\", load_tokenizer=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.previous_path = self.path\n",
        "      return self.model\n",
        "\n",
        "model_generator = Model()\n",
        "\n",
        "text1 = \"\"\"\n",
        "\"I recently purchased the Sony WH-1000XM4 Wireless Noise-Canceling Headphones from Amazon and I must say, I'm thoroughly impressed. The package arrived in New York within 2 days, thanks to Amazon Prime's expedited shipping.\n",
        "\n",
        "The headphones themselves are remarkable. The noise-canceling feature works like a charm in the bustling city environment, and the 30-hour battery life means I don't have to charge them every day. Connecting them to my Samsung Galaxy S21 was a breeze, and the sound quality is second to none.\n",
        "\n",
        "I also appreciated the customer service from Amazon when I had a question about the warranty. They responded within an hour and provided all the information I needed.\n",
        "\n",
        "However, the headphones did not come with a hard case, which was listed in the product description. I contacted Amazon, and they offered a 10% discount on my next purchase as an apology.\n",
        "\n",
        "Overall, I'd give these headphones a 4.5/5 rating and highly recommend them to anyone looking for top-notch quality in both product and service.\"\"\"\n",
        "\n",
        "\n",
        "text3 = \"\"\"\n",
        "Several studies have reported its pharmacological activities, including anti-inflammatory, antimicrobial, and antitumoral effects.\n",
        "The effect of E-anethole was studied in the osteosarcoma MG-63 cell line, and the antiproliferative activity was evaluated by an MTT assay.\n",
        "It showed a GI50 value of 60.25 μM with apoptosis induction through the mitochondrial-mediated pathway. Additionally, it induced cell cycle arrest at the G0/G1 phase, up-regulated the expression of p53, caspase-3, and caspase-9, and down-regulated Bcl-xL expression.\n",
        "Moreover, the antitumoral activity of anethole was assessed against oral tumor Ca9-22 cells, and the cytotoxic effects were evaluated by MTT and LDH assays.\n",
        "It demonstrated a LD50 value of 8 μM, and cellular proliferation was 42.7% and 5.2% at anethole concentrations of 3 μM and 30 μM, respectively.\n",
        "It was reported that it could selectively and in a dose-dependent manner decrease cell proliferation and induce apoptosis, as well as induce autophagy, decrease ROS production, and increase glutathione activity. The cytotoxic effect was mediated through NF-kB, MAP kinases, Wnt, caspase-3 and -9, and PARP1 pathways. Additionally, treatment with anethole inhibited cyclin D1 oncogene expression, increased cyclin-dependent kinase inhibitor p21WAF1, up-regulated p53 expression, and inhibited the EMT markers.\n",
        "\"\"\"\n",
        "\n",
        "text5 = \"\"\"\n",
        "Dr. Paul Hammond, a renowned neurologist at Johns Hopkins University, has recently published a paper in the prestigious journal \"Nature Neuroscience\". His research focuses on a rare genetic mutation, found in less than 0.01% of the population, that appears to prevent the development of Alzheimer's disease. Collaborating with researchers at the University of California, San Francisco, the team is now working to understand the mechanism by which this mutation confers its protective effect. Funded by the National Institutes of Health, their research could potentially open new avenues for Alzheimer's treatment.\n",
        "\"\"\"\n",
        "\n",
        "ner_examples = [\n",
        "    [\n",
        "        text5,\n",
        "        \"neurologist, scientist, gene, disease, biological process, city, journal, university\",\n",
        "        0.5,\n",
        "        False\n",
        "    ],\n",
        "    [\n",
        "        text1,\n",
        "        \"product, brand, location, features, rating\",\n",
        "        0.5,\n",
        "        False\n",
        "    ],\n",
        "    [\n",
        "        text3,\n",
        "        \"cell line, protein, metric, substance\",\n",
        "        0.5,\n",
        "        False\n",
        "    ]]\n",
        "\n",
        "def merge_entities(entities):\n",
        "    if not entities:\n",
        "        return []\n",
        "    merged = []\n",
        "    current = entities[0]\n",
        "    for next_entity in entities[1:]:\n",
        "        if next_entity['entity'] == current['entity'] and (next_entity['start'] == current['end'] + 1 or next_entity['start'] == current['end']):\n",
        "            current['word'] += ' ' + next_entity['word']\n",
        "            current['end'] = next_entity['end']\n",
        "        else:\n",
        "            merged.append(current)\n",
        "            current = next_entity\n",
        "    merged.append(current)\n",
        "    return merged\n",
        "\n",
        "def process(\n",
        "    model_name, text, labels: str, threshold: float, nested_ner: bool\n",
        ") -> Dict[str, Union[str, int, float]]:\n",
        "    model = model_generator.get_model(model_name)\n",
        "    labels = [label.strip() for label in labels.split(\",\")]\n",
        "    r = {\n",
        "        \"text\": text,\n",
        "        \"entities\": [\n",
        "            {\n",
        "                \"entity\": entity[\"label\"],\n",
        "                \"word\": entity[\"text\"],\n",
        "                \"start\": entity[\"start\"],\n",
        "                \"end\": entity[\"end\"],\n",
        "                \"score\": 0,\n",
        "            }\n",
        "            for entity in model.predict_entities(\n",
        "                text, labels, flat_ner=not nested_ner, threshold=threshold\n",
        "            )\n",
        "        ],\n",
        "    }\n",
        "    r[\"entities\"] =  merge_entities(r[\"entities\"])\n",
        "    return r\n",
        "\n",
        "#model = GLiNER.from_pretrained(f\"{MODEL_NAME}\", load_tokenizer=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with gr.Blocks(title=\"NER Task\") as ner_interface:\n",
        "    models = os.listdir(\"models\")\n",
        "    if not models:\n",
        "        print(\"No models found in the 'models' directory.\")\n",
        "    model_name = gr.Dropdown(label=\"Model Name\", choices=models)\n",
        "    input_text = gr.Textbox(label=\"Text input\", placeholder=\"Enter your text here\")\n",
        "    labels = gr.Textbox(label=\"Labels\", placeholder=\"Enter your labels here (comma separated)\", scale=2)\n",
        "    threshold = gr.Slider(0, 1, value=0.3, step=0.01, label=\"Threshold\", info=\"Lower the threshold to increase how many entities get predicted.\")\n",
        "    nested_ner = gr.Checkbox(label=\"Nested NER\", info=\"Allow for nested NER?\")\n",
        "    output = gr.HighlightedText(label=\"Predicted Entities\")\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    examples = gr.Examples(\n",
        "        ner_examples,\n",
        "        fn=process,\n",
        "        inputs=[input_text, labels, threshold, nested_ner],\n",
        "        outputs=output,\n",
        "        cache_examples=False\n",
        "    )\n",
        "    theme=gr.themes.Base()\n",
        "\n",
        "    input_text.submit(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    labels.submit(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    threshold.release(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    submit_btn.click(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "    nested_ner.change(fn=process, inputs=[model_name, input_text, labels, threshold, nested_ner], outputs=output)\n",
        "\n",
        "ner_interface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MqNq2pjJ3tLN"
      },
      "outputs": [],
      "source": [
        "# @title Upload model to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def zip_directory(model_name):\n",
        "    model_path = f\"models/{model_name}\"\n",
        "    zip_path = f\"{model_path}.zip\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for root, dirs, files in os.walk(model_path):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, start=model_path)\n",
        "                    zipf.write(file_path, arcname)\n",
        "        return zip_path\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def upload_to_drive(zip_path, drive_folder='My Drive'):\n",
        "    if zip_path and os.path.exists(zip_path):\n",
        "        destination_dir = f'/content/drive/{drive_folder}'\n",
        "        os.makedirs(destination_dir, exist_ok=True)\n",
        "        destination = f'{destination_dir}/{os.path.basename(zip_path)}'\n",
        "        shutil.move(zip_path, destination)\n",
        "        return f\"File uploaded to {destination}\"\n",
        "    else:\n",
        "        return \"Zip file not found.\"\n",
        "\n",
        "def zip_and_upload(model_name, drive_path):\n",
        "    zip_path = zip_directory(model_name)\n",
        "    if zip_path:\n",
        "        upload_message = upload_to_drive(zip_path, drive_folder=drive_path)\n",
        "        return f\"Directory '{model_name}' zipped successfully as '{zip_path}'. {upload_message}\"\n",
        "    else:\n",
        "        return f\"Directory '{model_name}' not found.\"\n",
        "\n",
        "with gr.Blocks() as to_drive:\n",
        "    gr.Markdown(\"# GLiNER Model Zipper and Uploader\")\n",
        "\n",
        "    models = os.listdir(\"models\")\n",
        "    model_name = gr.Dropdown(label=\"Choose the model\", choices=models, value=models[0])\n",
        "    drive_path = gr.Textbox(label=\"Google Drive Path\", placeholder=\"Enter the path on Google Drive (e.g., 'My Drive/Models')\", value='My Drive/Models')\n",
        "    upload_btn = gr.Button(\"Zip and Upload Model\")\n",
        "    output_info = gr.Textbox(label=\"Output Info\")\n",
        "    upload_btn.click(fn=zip_and_upload, inputs=[model_name, drive_path], outputs=output_info)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "to_drive.launch(inline=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a39ad6349a4e42eeab9cd9802f728abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30ba3ae0227e434f9bb01c93890fa812",
              "IPY_MODEL_790a1064f9e74c8ba1a7a2a9d8aabf32",
              "IPY_MODEL_c1f258dd5ba6476ba4dc83dd5d5a48bd"
            ],
            "layout": "IPY_MODEL_d7af1df2c7224a428a035c31f827dc4f"
          }
        },
        "30ba3ae0227e434f9bb01c93890fa812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a60b93d225c04c3ea6b3d23a203bf5ad",
            "placeholder": "​",
            "style": "IPY_MODEL_0e6b7aee22a5476da838ba6e57797382",
            "value": "Fetching 10 files: 100%"
          }
        },
        "790a1064f9e74c8ba1a7a2a9d8aabf32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9162131de7644e2db20ab1c331d3f4db",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07b47cced2224025825f849c57567ea9",
            "value": 10
          }
        },
        "c1f258dd5ba6476ba4dc83dd5d5a48bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbc910057a8547f2b3ef6e5ab36fefbd",
            "placeholder": "​",
            "style": "IPY_MODEL_8eb955ec9be045629e361bbddaacbc0c",
            "value": " 10/10 [00:00&lt;00:00, 281.60it/s]"
          }
        },
        "d7af1df2c7224a428a035c31f827dc4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60b93d225c04c3ea6b3d23a203bf5ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e6b7aee22a5476da838ba6e57797382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9162131de7644e2db20ab1c331d3f4db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07b47cced2224025825f849c57567ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbc910057a8547f2b3ef6e5ab36fefbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb955ec9be045629e361bbddaacbc0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}